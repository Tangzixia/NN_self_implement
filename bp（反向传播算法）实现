'''这儿需要大家掌握的可能就是BP的4个公式，具体大家可以参考机器学习和神经网络那本书中的只是，
主要是每一层的误差和根据误差求得loss函数对权重w和偏置b的偏导数，
然后更重要的是具体的代码实现，即就是矩阵的运算，具体的实现包括矩阵的格式，是否需要对矩阵进行转置操作。'''
#-*-coding=utf-8-*-
import numpy as np
import tensorflow as tf
#sizes=[3,4,3]
#w=[[4*3],[3*4]]
#b=[[4*1],[3*1]]
#a=[[4,],[3,]]
#z=[[4,],[3,]]

def sigmoid(z):
	return 1.0/(1.0+np.exp(-z))

def sigmoid_prime(z):
	return sigmoid(z)*(1-sigmoid(z))

class Network(object):
	def __init__(self,sizes):
		self.num_layers=len(sizes)
		self.sizes=sizes
		self.weights=[np.random.rand(j,i) for i,j in zip(sizes[:-1],sizes[1:])]
		self.biases=[np.random.rand(j,1) for j in sizes[1:]]

	def feedforward(self,a):
		for w,b in zip(self.weights,self.biases):
			a=sigmoid(np.dot(w,a)+b)
		return a

	def backprop(self,x,y):
		deriv_b=[np.zeros(b.shape) for b in self.biases]
		deriv_w=[np.zeros(w.shape) for w in self.weights]

		a=x
		a_arr=[x]
		z_arr=[]
		for w,b in zip(self.weights,self.biases):
			print(np.dot(w,a).shape,b.shape)
			z=np.dot(w,a)+b
			z_arr.append(z)
			a=sigmoid(z)
			a_arr.append(a)

		print(a_arr[-1].shape)
		delta=(a_arr[-1]-y)*sigmoid_prime(z_arr[-1])

		deriv_b[-1]=delta
		deriv_w[-1]=np.dot(delta,np.transpose(a_arr[-2]))

		for l in xrange(2,self.num_layers):
			z=z_arr[-l]


			delta=np.dot(np.transpose(self.weights[-l+1]),delta)*sigmoid_prime(z)
			print(delta.shape,a_arr[-l-1].shape)
			deriv_b[-l]=delta
			deriv_w[-l]=np.dot(delta,np.transpose(a_arr[-l-1]))

		return (deriv_w,deriv_b)

if __name__=="__main__":
	sizes=[3,4,3]

	#the model (len(n1_node)*1) format to start with
	
	x=[1,2,3]

	x=np.expand_dims(x,1)
	y=np.random.rand(3)
	y=np.expand_dims(y,1)
	z=[(x,y)]
	net1=Network(sizes)
	(w,b)=net1.backprop(z[0][0],z[0][1])
	
	print(w[0].shape,w[1].shape)
	print(w[0],w[1])
	print(b[0].shape,b[1].shape)
	print(b[0],b[1])
